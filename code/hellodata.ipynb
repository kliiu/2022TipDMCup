{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#本程序主要用于jieba分词，以及去除停用词\n",
    " \n",
    "import os\n",
    "import jieba\n",
    " \n",
    "# 保存文件的函数\n",
    "def savefile(savepath,content):\n",
    "    fp = open(savepath,'w',encoding='utf8',errors='ignore')\n",
    "    fp.write(content)\n",
    "    fp.close()\n",
    " \n",
    "# 读取文件的函数\n",
    "def readfile(path):\n",
    "    fp = open(path, \"r\", encoding='utf8', errors='ignore')\n",
    "    content = fp.read()\n",
    "    fp.close()\n",
    "    return content\n",
    " \n",
    "## 去除停用词的2个函数\n",
    "# 创建停用词list\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='gb18030').readlines()]\n",
    "    return stopwords\n",
    " \n",
    "# 对句子去除停用词\n",
    "def movestopwords(sentence):\n",
    "    stopwords = stopwordslist('stop_words.txt')  # 这里加载停用词的路径\n",
    "    outstr = ''\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t'and'\\n':\n",
    "                outstr += word\n",
    "                # outstr += \" \"\n",
    "    return outstr\n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    corpus_path = \"语料/train/\"  # 未分词分类预料库路径\n",
    "    seg_path = \"语料/train_seg/\"  # 分词后分类语料库路径\n",
    " \n",
    "    catelist = os.listdir(corpus_path)  # 获取未分词目录下所有子目录\n",
    "    for mydir in catelist:\n",
    "        class_path = corpus_path + mydir + \"/\"  # 拼出分类子目录的路径\n",
    "        seg_dir = seg_path + mydir + \"/\"  # 拼出分词后预料分类目录\n",
    "        if not os.path.exists(seg_dir):  # 是否存在，不存在则创建\n",
    "            os.makedirs(seg_dir)\n",
    " \n",
    "        file_list = os.listdir(class_path) # 列举当前目录所有文件\n",
    "        for file_path in file_list:\n",
    "            fullname = class_path + file_path # 路径+文件名\n",
    "            print(\"当前处理的文件是： \",fullname)  # 语料/train/pos/pos.txt\n",
    "                            #  语料/train/neg/neg1.txt\n",
    " \n",
    "            content = readfile(fullname).strip()  # 读取文件内容\n",
    "            content = content.replace(\"\\n\", \"\").strip()  # 删除换行和多余的空格\n",
    "            content_seg = jieba.cut(content)    # jieba分词\n",
    "            print(\"jieba分词后：\",content_seg)\n",
    "            listcontent = ''\n",
    "            for i in content_seg:\n",
    "                listcontent += i\n",
    "                listcontent += \" \"\n",
    "                # listcontent.replace(' ','\\n').replace('  ','\\n')\n",
    "            print(listcontent[0:10])\n",
    "            listcontent = movestopwords(listcontent)    # 去除停用词\n",
    "            print(\"去除停用词后：\", listcontent[0:10])\n",
    "            listcontent = listcontent.replace(\"   \", \"\\n\").replace(\"  \", \"\\n\")\n",
    "            savefile(seg_dir + file_path, \"\".join(listcontent)) # 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687a7caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>文章ID</th>\n",
       "      <th>文章标题</th>\n",
       "      <th>公众号文章内容</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>文章-0001</td>\n",
       "      <td>袁古洁到高新区调研：坚定信心谋发展，扎实推动高新区产业实现高质量发展</td>\n",
       "      <td>&gt;&gt;&gt;\\n\\n11月5日，市委书记、市人大常委会主任袁古洁到高新区调研经济社会发展、重点项目...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>文章-0002</td>\n",
       "      <td>【全民反诈】敲屏幕！国家反诈中心App，您下载吗？</td>\n",
       "      <td>近年来\\n电信网络诈骗案件高发\\n\\n国家反诈中心App能干啥？\\n往下看，带您了解！\\n▼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>文章-0003</td>\n",
       "      <td>钟南山最新研判：接种第三针新冠疫苗很有必要，老年人尽快接种！</td>\n",
       "      <td>11月5日，“共和国勋章”获得者、中国工程院院士钟南山带领团队到深圳市人民医院交流指导。\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>文章-0004</td>\n",
       "      <td>向榜样致敬！第八届全国道德模范提名奖获得者载誉归茂</td>\n",
       "      <td>“院长妈妈好，您辛苦了！”\\n\\n  11月6日晚，\\n第八届全国道德模范提名奖获得者李兰\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>文章-0005</td>\n",
       "      <td>【全民反诈】警方提醒：缅北已成诈骗团伙主要匿身之地</td>\n",
       "      <td>“那些人没有人性\\n如果你在那边不配合工作\\n不按照他们的要求去骗人\\n你就会被打 \\n耳光...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>文章-0076</td>\n",
       "      <td>5月20日聚焦茂名，享一场荔枝盛会！</td>\n",
       "      <td>5月20日，由农业农村部南亚热带作物中心、国家荔枝龙眼产业技术体系、广东省农业农村厅和茂名市...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>文章-0077</td>\n",
       "      <td>给“荔”！5·19中国旅游日 （茂名）暨‘我爱荔’旅游季启动</td>\n",
       "      <td>今天上午，由广东省旅游协会、茂名市文化广电旅游体育局指导,由市旅游产业商会主办的“5·19中...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>文章-0078</td>\n",
       "      <td>政府工作报告（文字实录）</td>\n",
       "      <td>政府工作报告（文字实录）\\n——2020年5月22日在第十三届全国人民代表大会第三次会议上\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>文章-0079</td>\n",
       "      <td>最全！一图读懂2020年《政府工作报告》</td>\n",
       "      <td>最全版来啦！\\n2020年《政府工作报告》一图全读懂\\n快些收藏！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>文章-0080</td>\n",
       "      <td>【聚焦两会】许志晖：茂名主导产业发展并未“暂停”反而“加速”</td>\n",
       "      <td>建市以来最大民营工业项目动工建设，医用口罩生产企业“从无到有”，大健康产业加快布局……全国人...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       文章ID                                文章标题  \\\n",
       "0   文章-0001  袁古洁到高新区调研：坚定信心谋发展，扎实推动高新区产业实现高质量发展   \n",
       "1   文章-0002           【全民反诈】敲屏幕！国家反诈中心App，您下载吗？   \n",
       "2   文章-0003      钟南山最新研判：接种第三针新冠疫苗很有必要，老年人尽快接种！   \n",
       "3   文章-0004           向榜样致敬！第八届全国道德模范提名奖获得者载誉归茂   \n",
       "4   文章-0005           【全民反诈】警方提醒：缅北已成诈骗团伙主要匿身之地   \n",
       "..      ...                                 ...   \n",
       "75  文章-0076                  5月20日聚焦茂名，享一场荔枝盛会！   \n",
       "76  文章-0077      给“荔”！5·19中国旅游日 （茂名）暨‘我爱荔’旅游季启动   \n",
       "77  文章-0078                        政府工作报告（文字实录）   \n",
       "78  文章-0079                最全！一图读懂2020年《政府工作报告》   \n",
       "79  文章-0080      【聚焦两会】许志晖：茂名主导产业发展并未“暂停”反而“加速”   \n",
       "\n",
       "                                              公众号文章内容  \n",
       "0   >>>\\n\\n11月5日，市委书记、市人大常委会主任袁古洁到高新区调研经济社会发展、重点项目...  \n",
       "1   近年来\\n电信网络诈骗案件高发\\n\\n国家反诈中心App能干啥？\\n往下看，带您了解！\\n▼...  \n",
       "2   11月5日，“共和国勋章”获得者、中国工程院院士钟南山带领团队到深圳市人民医院交流指导。\\n...  \n",
       "3   “院长妈妈好，您辛苦了！”\\n\\n  11月6日晚，\\n第八届全国道德模范提名奖获得者李兰\\...  \n",
       "4   “那些人没有人性\\n如果你在那边不配合工作\\n不按照他们的要求去骗人\\n你就会被打 \\n耳光...  \n",
       "..                                                ...  \n",
       "75  5月20日，由农业农村部南亚热带作物中心、国家荔枝龙眼产业技术体系、广东省农业农村厅和茂名市...  \n",
       "76  今天上午，由广东省旅游协会、茂名市文化广电旅游体育局指导,由市旅游产业商会主办的“5·19中...  \n",
       "77  政府工作报告（文字实录）\\n——2020年5月22日在第十三届全国人民代表大会第三次会议上\\...  \n",
       "78                  最全版来啦！\\n2020年《政府工作报告》一图全读懂\\n快些收藏！  \n",
       "79  建市以来最大民营工业项目动工建设，医用口罩生产企业“从无到有”，大健康产业加快布局……全国人...  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"sample.xlsx\"\n",
    "df= pd.read_excel(path,sheet_name=4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[(df['景区名称']==jingqu_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed851c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(len(df))\n",
    "df.drop_duplicates(subset=['评论内容'],keep='first',inplace=True)#简单去重\n",
    "#print(len(df))\n",
    "\n",
    "save_path=jingqu_name+'.txt'\n",
    "df.to_csv(save_path,header=0,index_label=False,index=False, encoding='utf-8',columns=['评论内容'])#将评论保存为纯文本文件\n",
    " # 给出文档路径\n",
    "filename = save_path\n",
    "outfilename = \"分词-\"+save_path\n",
    "inputs = open(filename, 'r',encoding='utf-8')\n",
    "outputs = open(outfilename, 'w',encoding='utf-8')\n",
    "\n",
    "# 将输出结果写入out.txt中\n",
    "for line in inputs:\n",
    "    line_seg = seg_depart(line)\n",
    "    outputs.write(line_seg + '\\n')\n",
    "print(\"-------------------正在分词和去停用词-----------\")\n",
    "outputs.close()\n",
    "inputs.close()\n",
    "print(\"删除停用词和分词成功！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3_1(jingqu_name):#景区\n",
    "    path=\"sample.xlsx\"\n",
    "    df= pd.read_excel(path,sheet_name=4)\n",
    "    df=df[(df['景区名称']==jingqu_name)]\n",
    "    #print(len(df))\n",
    "    df.drop_duplicates(subset=['评论内容'],keep='first',inplace=True)#简单去重\n",
    "    #print(len(df))\n",
    "    \n",
    "    save_path=jingqu_name+'.txt'\n",
    "    df.to_csv(save_path,header=0,index_label=False,index=False, encoding='utf-8',columns=['评论内容'])#将评论保存为纯文本文件\n",
    "     # 给出文档路径\n",
    "    filename = save_path\n",
    "    outfilename = \"分词-\"+save_path\n",
    "    inputs = open(filename, 'r',encoding='utf-8')\n",
    "    outputs = open(outfilename, 'w',encoding='utf-8')\n",
    "\n",
    "    # 将输出结果写入ou.txt中\n",
    "    for line in inputs:\n",
    "        line_seg = seg_depart(line)\n",
    "        outputs.write(line_seg + '\\n')\n",
    "    print(\"-------------------正在分词和去停用词-----------\")\n",
    "    outputs.close()\n",
    "    inputs.close()\n",
    "    print(\"删除停用词和分词成功！！！\")\n",
    "\n",
    "# 创建停用词列表\n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open('../stopwords/hit_stopwords.txt',encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "# 对句子进行中文分词\n",
    "def seg_depart(sentence):\n",
    "    # 对文档中的每一行进行中文分词\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    # 创建一个停用词列表\n",
    "    stopwords = stopwordslist()\n",
    "    # 输出结果为outstr\n",
    "    outstr = ''\n",
    "    # 去停用词\n",
    "    for word in sentence_depart:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x=1\n",
    "    while(x!=3):\n",
    "        if(x<10):\n",
    "            jingqu_name='A0'+str(x)\n",
    "        else:\n",
    "            jingqu_name='A'+str(x)\n",
    "        x+=1\n",
    "        task3_1(jingqu_name)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pyto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
